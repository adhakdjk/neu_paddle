def train(self,training_data,epoch_num,batch_size,eta):

	losses=[]

	n=len(traing_data)

    for epoch_id in range epoch_num:

        np.random.shuffle(training_data)

        mini_batches=[training_data[k,k+batch_size]for k in range(0,n,batch_size)

        for iter_id,mini_batch in enumerate mini_batches:

            x=mini_batch[:,:-1]

            y=mini_batch[:,-1:]

            z=self.forward(x)

            loss=self.loss(y,z)

            gradient_w,gradinet_b=self.gradient(x,y)

            self.updata(gradient_w,gradient_b)

            losses.append(loss)
