{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%python -m spacy download de_core_news_sm\n",
    "%python -m spacy download en_core_web_sm\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax,pad\n",
    "import math \n",
    "import copy\n",
    "import time \n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import pandas as pd \n",
    "import altair as alt\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings \n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整个notebook的便利函数\n",
    "def is_interactive_notebook():\n",
    "    return __name__==\"__main__\"\n",
    "\n",
    "def show_example(fn,args=[]):\n",
    "    if __name__==\"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "def execute_example(fn,args=[]):\n",
    "    if __name__==\"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups=[{\"lr\":0}]\n",
    "        None\n",
    "    def step(self):\n",
    "        None\n",
    "    def zero_grad(self,set_to_none=False):\n",
    "        None\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,encoder,decoder,src_embed,tgt_embed,generator) -> None:\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_embed=src_embed\n",
    "        self.tgt_embed=tgt_embed\n",
    "        self.generator=generator\n",
    "\n",
    "    def foward(self,src,tgt,src_mask,tgt_mask):\n",
    "         return self.decoder(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
    "    #      \"Take in and process masked src and target sequences.\"\n",
    "    def encode(self,src,src_mask):\n",
    "        return self.encoder(self.src_embed(src),src_mask)\n",
    "    \n",
    "    def decoder(self,memory,src_mask,tgt,tgt_mask):\n",
    "        \n",
    "        return self.decoder(self.tgt_embed(tgt),memory,src_mask,tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # 定义linear+softmax generation step\n",
    "    def __init__(self,d_model,vocab) -> None:\n",
    "        super(Generator,self).__init__()\n",
    "        self.proj=nn.Linear(d_model,vocab)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return log_softmax(self.proj(x),dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clones(module,N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)  ])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Some Information about Encoder\"\"\"\n",
    "    def __init__(self,layer,N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers=clones(layer,N)\n",
    "        self.norm=LayerNorm(layer.size)\n",
    "    def forward(self, x,mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Some Information about LayerNorm\"\"\"\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 =nn.Parameter(torch.ones(features))\n",
    "        self.b_2=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "    def forward(self, x):\n",
    "        mean=x.mean(-1,keepdim=True)\n",
    "        std=x.std(-1,keepdim=True)\n",
    "\n",
    "        return self.a_2*(x-mean)/(std+self.eps)+self.b_2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"Some Information about SublayerConnection\"\"\"\n",
    "    def __init__(self,size,dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm=LayerNorm(size)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self, x,sublayer):\n",
    "        return x+self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder is made up of self-attn and feed forward (defined below)\"\"\"\n",
    "    def __init__(self,size,self_attn,feed_forward,dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn=self_attn\n",
    "        self.feed_forward=feed_forward\n",
    "        self.sublayer=clones(SublayerConnection(size,dropout),2)\n",
    "        self.size=size\n",
    "    def forward(self, x,mask):\n",
    "        x=self.sublayer[0](x,lambda x: self.self_attn(x,x,x,mask))\n",
    "\n",
    "        return self.sublayer[1](x,self.feed_forward)\n",
    "    \n",
    "    # Each layer has two sub-layers. \n",
    "    # The first is a multi-head self-attention mechanism, \n",
    "    # and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Some Information about Decoder\"\"\"\n",
    "    def __init__(self,layer,N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers=clones(layer,N)\n",
    "        self.norm=LayerNorm(layer.size)\n",
    "\n",
    "\n",
    "    def forward(self, x,memory,src_mask,tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,memory,src_mask,tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Some Information about DecoderLayer\"\"\"\n",
    "    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size=size\n",
    "        self.self_attn=src_attn\n",
    "        self.src_attn=src_attn\n",
    "        self.feed_forward=feed_forward\n",
    "        self.dropout=dropout\n",
    "        self.sublayer=clones(SublayerConnection(size,dropout,3))\n",
    "\n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        m=memory\n",
    "        x=self.sublayer[0](x,lambda x: self.self_attn(x,x,x,tgt_mask))\n",
    "        x=self.sublayer[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n",
    "        return self.sublayer[2](x,self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    # 修改自注意力层 防止关注到位置之后的内容\n",
    "    attn_shape=(1,size,size)\n",
    "    subsequent_mask=torch.triu(torch.ones(attn_shape),diagonal=1).type(torch.uint8)\n",
    "    return subsequent_mask==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_mask():\n",
    "    LS_data=pd.concat(\n",
    "        [pd.DataFrame(\n",
    "            {\n",
    "                \"Subsequent Mask\": subsequent_mask(20)[0][x,y].flatten(),\n",
    "                \"Window\": x,\n",
    "                \"Masking\": y,\n",
    "            }\n",
    "        )\n",
    "        for y in range(20)\n",
    "        for x in range(20)\n",
    "        ]\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect()\n",
    "        .properties(height=250,windth=250)\n",
    "        .\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cffdb135bfca6e4a47037eb880605a7800674b0a8ee5d1d049ded49b6c5e4e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
