{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"SimHei\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "# 定义sigmoid函数。\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "z = np.linspace(-10, 10, 200)\n",
    "plt.plot(z, sigmoid(z))\n",
    "# 绘制水平线与垂直线。\n",
    "plt.axvline(x=0, ls=\"--\", c=\"k\")\n",
    "plt.axhline(ls=\":\", c=\"k\")\n",
    "plt.axhline(y=0.5, ls=\":\", c=\"k\")\n",
    "plt.axhline(y=1, ls=\":\", c=\"k\")\n",
    "plt.xlabel(\"z值\")\n",
    "plt.ylabel(\"sigmoid(z)值\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于BERT实现地址要素解析比赛\n",
    "\n",
    "## （1）赛题描述\n",
    "中文地址要素解析任务的目标即将一条地址分解为上述几个部分的详细标签，如：\n",
    "\n",
    "输入：浙江省杭州市余杭区五常街道文一西路969号淘宝城5号楼，放前台\n",
    "\n",
    "输出：Province=浙江省 city=杭州市 district=余杭区 town=五常街道 road=文一西路road_number=969号 poi=淘宝城 house_number=5号楼 other=，放前台\n",
    "\n",
    "## （2）数据说明\n",
    "标注数据集由训练集、验证集和测试集组成，整体标注数据大约2万条左右。地址数据通过抓取公开的地址信息（如黄页网站等）获得， 均通过众包标注生成，详细标注规范将会在数据发布时一并给出。\n",
    "\n",
    "## （3）命名实体识别介绍\n",
    "命名实体识别是NLP中一项非常基础的任务，是信息提取、问答系统、句法分析、机器翻译等众多NLP任务的重要基础工具。命名实体识别的准确度，决定了下游任务的效果，是NLP中的一个基础问题。在NER任务提供了两种解决方案，一类LSTM/GRU + CRF，RNN类的模型来抽取底层文本的信息，而CRF(条件随机场)模型来学习底层Token之间的联系；另外一类是通过预训练模型，例如ERNIE，BERT模型，直接来预测Token的标签信息。本文采用两者结合的方法。\n",
    "\n",
    "本项目将演示，如何使用从快递单中抽取姓名、电话、省、市、区、详细地址等内容，形成结构化信息。辅助物流行业从业者进行有效信息的提取，从而降低客户填单的成本，完成比赛。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch URL https://pypi.org/simple/paddlenlp/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/paddlenlp/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))) - skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/paddlenlp/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/paddlenlp/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/paddlenlp/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/paddlenlp/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/paddlenlp/\n",
      "ERROR: Could not find a version that satisfies the requirement paddlenlp (from versions: none)\n",
      "ERROR: No matching distribution found for paddlenlp\n"
     ]
    }
   ],
   "source": [
    "#安装paddlenlp2.0\n",
    "#!pip install paddlenlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1、导入必要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Pad, Tuple\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "from functools import partial #partial()函数可以用来固定某些参数值，并返回一个新的callable对象\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2、数据查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n10 dataset/train.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n10  dataset/dev.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head dataset/final_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3、数据格式调整，将原先每行是每个字的标注形式，修改为每行是每句话的标注形式，相邻字（标注）之间，采用符号'\\002'进行分隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#对文件source_filename的数据格式进行调整，结果保存在文件target_filename中\n",
    "def format_data(source_filename, target_filename):\n",
    "    #pdb.set_trace()\n",
    "    #结果列表初始化为空\n",
    "    datalist=[]\n",
    "    #读取source_filename所有数据到lines中，每个元素是字标注\n",
    "    with open(source_filename, 'r', encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "    words=''\n",
    "    labels=''\n",
    "    #当前处理的是否为每句话首字符，0：是，1：不是\n",
    "    flag=0\n",
    "    #逐个处理每个字标注\n",
    "    for line in lines:\n",
    "        #空行表示每句话标注的结束\n",
    "        if line == '\\n':\n",
    "            #连接文本和标注结果\n",
    "            item=words+'\\t'+labels+'\\n'\n",
    "            #print(item)\n",
    "            #添加到结果列表中\n",
    "            datalist.append(item)\n",
    "            #重置文本和标注结果\n",
    "            words=''\n",
    "            labels=''\n",
    "            flag=0\n",
    "            continue\n",
    "        #pdb.set_trace()\n",
    "        #分离出字和标注\n",
    "        word, label = line.strip('\\n').split(' ')\n",
    "        #不是每句话的首字符\n",
    "        if flag==1:\n",
    "            #words/labels非空，和字/标签连接时需要添加分隔符'\\002'\n",
    "            words=words+'\\002'+word\n",
    "            labels=labels+'\\002'+label\n",
    "        else:#每句话首字符，words/labels为空，和字/标签连接时不需要添加分隔符'\\002'\n",
    "            words=words+word\n",
    "            labels=labels+label\n",
    "            flag=1#修改标志\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\n",
    "        #pdb.set_trace()\n",
    "        #将转换结果写入文件target_filename\n",
    "        lines=f.writelines(datalist)\n",
    "    print(f'{source_filename}文件格式转换完毕，保存为{target_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#逐个转换文件\n",
    "format_data('./dataset/dev.conll', './dataset/dev.txt')\n",
    "format_data(r'./dataset/train.conll', r'./dataset/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head dataset/dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4、构建Label标签表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#提取文件source_filename1和source_filename2的标签类型，保存到target_filename\n",
    "def gernate_dic(source_filename1, source_filename2, target_filename):\n",
    "    #标签类型列表初始化为空\n",
    "    data_list=[]\n",
    "\n",
    "    #读取source_filename1所有行到lines中，每行元素是单字和标注\n",
    "    with open(source_filename1, 'r', encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    #处理每行数据（单字+‘ ’+标注）\n",
    "    for line in lines:\n",
    "        #数据非空\n",
    "        if line != '\\n':\n",
    "            #提取标注，-1是数组最后1个元素\n",
    "            dic=line.strip('\\n').split(' ')[-1]\n",
    "            #不在标签类型列表中，则添加\n",
    "            if dic+'\\n' not in data_list:\n",
    "                data_list.append(dic+'\\n')\n",
    "    \n",
    "    #读取source_filename2所有行到lines中，每行元素是单字和标注\n",
    "    with open(source_filename2, 'r', encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "\n",
    "    #处理每行数据（单字+‘ ’+标注）\n",
    "    for line in lines:\n",
    "        #数据非空\n",
    "        if line != '\\n':\n",
    "            #提取标注，-1是数组最后1个元素\n",
    "            dic=line.strip('\\n').split(' ')[-1]\n",
    "            #不在标签类型列表中，则添加\n",
    "            if dic+'\\n' not in data_list:\n",
    "                data_list.append(dic+'\\n')\n",
    "\n",
    "    with open(target_filename, 'w', encoding='utf-8') as f:\n",
    "        #将标签类型列表写入文件target_filename\n",
    "        lines=f.writelines(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 根据训练集和验证集生成dic，保存所有的标签\n",
    "gernate_dic('dataset/train.conll', 'dataset/dev.conll', 'dataset/mytag.dic')\n",
    "# 查看生成的dic文件\n",
    "!cat dataset/mytag.dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5、加载自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载数据文件datafiles\n",
    "def load_dataset(datafiles):\n",
    "    #读取数据文件data_path\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            next(fp)  # Skip header  #Deleted by WGM\n",
    "            #处理每行数据（文本+‘\\t’+标注）\n",
    "            for line in fp.readlines():\n",
    "                #提取文本和标注\n",
    "                words, labels = line.strip('\\n').split('\\t')\n",
    "                #文本中单字和标注构成的数组\n",
    "                words = words.split('\\002')\n",
    "                labels = labels.split('\\002')\n",
    "                #迭代返回文本和标注\n",
    "                yield words, labels\n",
    "    \n",
    "    #根据datafiles的数据类型，选择合适的处理方式\n",
    "    if isinstance(datafiles, str):#字符串，单个文件名称\n",
    "        #返回单个文件对应的单个数据集\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):#列表或元组，多个文件名称\n",
    "        #返回多个文件对应的多个数据集\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "#加载字典文件，文件由单列构成，需要设置value\n",
    "def load_dict_single(dict_path):\n",
    "    #字典初始化为空\n",
    "    vocab = {}\n",
    "    #value是自增数值，从0开始\n",
    "    i = 0\n",
    "    #逐行读取字典文件\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        #将每行文字设置为key\n",
    "        key = line.strip('\\n')\n",
    "        #设置对应的value\n",
    "        vocab[key] = i\n",
    "        i+=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 加载Bert模型需要的输入数据\n",
    "train_ds, dev_ds = load_dataset(datafiles=(\n",
    "        './dataset/train.txt', './dataset/dev.txt'))\n",
    "#加载标签文件，并转换为KV表，K为标签，V为编号（从0开始递增）\n",
    "label_vocab = load_dict_single('./dataset/mytag.dic')\n",
    "\n",
    "print(\"训练集、验证集、测试集的数量：\")\n",
    "print(len(train_ds),len(dev_ds))\n",
    "print(train_ds[0])\n",
    "print(dev_ds[0])\n",
    "print(label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6、数据集预处理，转换为Bert模型可以接受的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "#tokenizer：预编码器，label_vocab：标签类型KV表，K是标签类型，V是编码\n",
    "def convert_example(example,tokenizer,label_vocab,max_seq_length=256,is_test=False):\n",
    "    #测试集没有标签\n",
    "    if is_test:\n",
    "        text = example\n",
    "    else:#训练集和验证集包含标签\n",
    "        text, label = example\n",
    "    #tokenizer.encode方法能够完成切分token，映射token ID以及拼接特殊token\n",
    "    encoded_inputs = tokenizer.encode(text=text, max_seq_len=None, pad_to_max_seq_len=False, return_length=True)\n",
    "    #pdb.set_trace()\n",
    "    #获取字符编码（'input_ids'）、类型编码（'token_type_ids'）、字符串长度（'seq_len'）\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    segment_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    seq_len = encoded_inputs[\"seq_len\"]\n",
    "\n",
    "    if not is_test:#训练集和验证集\n",
    "        #[CLS]和[SEP]对应的标签均是['O']，添加到标签序列中\n",
    "        label = ['O'] + label + ['O']\n",
    "        #生成由标签编码构成的序列\n",
    "        label = [label_vocab[x] for x in label]\n",
    "        return input_ids, segment_ids, seq_len, label\n",
    "    else:#测试集，不返回标签序列\n",
    "        return input_ids, segment_ids, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载Bert预训练模型，将原始输入文本转化成序列标注模型model可接受的输入数据格式。\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "#functools.partial()的功能：预先设置参数，减少使用时设置的参数个数\n",
    "#使用partial()来固定convert_example函数的tokenizer, label_vocab, max_seq_length等参数值\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_length=128)\n",
    "\n",
    "\n",
    "#对训练集和测试集进行编码\n",
    "train_ds.map(trans_func)\n",
    "dev_ds.map(trans_func)\n",
    "print ([train_ds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用paddle.io.DataLoader接口多线程异步加载数据。\n",
    "ignore_label = -1\n",
    "#创建Tuple对象，将多个批处理函数的处理结果连接在一起\n",
    "#因为数据集train_ds、dev_ds的每条数据包含4部分，所以Tuple对象中包含4个批处理函数\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    #将每条数据的input_ids组合为数组，如果input_ids不等长，那么填充为pad_val\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "    #将每条数据的segment_ids组合为数组，如果segment_ids不等长，那么填充为pad_val\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "    #将每条数据的seq_len组合为数组\n",
    "    Stack(),\n",
    "    #将每条数据的label组合为数组，如果label不等长，那么填充为pad_val\n",
    "    Pad(axis=0, pad_val=ignore_label)\n",
    "): fn(samples)\n",
    "\n",
    "#paddle.io.DataLoader加载给定数据集，返回迭代器，每次迭代访问batch_size条数据\n",
    "#使用collate_fn定义所读取数据的格式\n",
    "#训练集\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=300,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "#验证集\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=300,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 7、Bert+BiGRU+CRF模型和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "from paddlenlp.transformers import  BertModel\n",
    "from paddlenlp.layers.crf import LinearChainCrf, LinearChainCrfLoss, ViterbiDecoder\n",
    "#继承nn.Layer才能训练时call forward函数\n",
    "class BertBiGRUCRFForTokenClassification(nn.Layer):\n",
    "    def __init__(self,bert,gru_hidden_size=300,\n",
    "                num_class=2,\n",
    "                crf_lr=100):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_class\n",
    "        self.bert = bert\n",
    "        self.gru = nn.GRU(self.bert.config[\"hidden_size\"],\n",
    "                          gru_hidden_size,\n",
    "                          num_layers = 2,\n",
    "                          direction='bidirect')\n",
    "        self.fc = nn.Linear(gru_hidden_size * 2, self.num_classes + 2)\n",
    "        self.crf = LinearChainCrf(self.num_classes)\n",
    "        self.crf_loss = LinearChainCrfLoss(self.crf)\n",
    "        self.viterbi_decoder = ViterbiDecoder(self.crf.transitions)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids,\n",
    "                lengths=None,\n",
    "                labels=None):\n",
    "        sequence_out, _ = self.bert(input_ids,\n",
    "                                    token_type_ids=token_type_ids)\n",
    "        gru_output, _ = self.gru(sequence_out)\n",
    "        emission = self.fc(gru_output)# 参数写错了，查了很长时间。\n",
    "        if labels is not None:\n",
    "            loss = self.crf_loss(emission, lengths, labels)\n",
    "            return loss\n",
    "        else:\n",
    "            _, prediction = self.viterbi_decoder(emission, lengths)\n",
    "            return prediction\n",
    "bert = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertBiGRUCRFForTokenClassification(bert, 300,len(label_vocab),100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#设置Fine-Tune优化策略\n",
    "#1.计算了块检测的精确率、召回率和F1-score。常用于序列标记任务，如命名实体识别\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "#2.在Adam的基础上加入了权重衰减的优化器，可以解决L2正则化失效问题\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())\n",
    "#损失函数由模型给出\n",
    "#3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#评估函数\n",
    "def evaluate(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()#评估器复位\n",
    "    #依次处理每批数据\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        #CRF Loss\n",
    "        preds = model(input_ids, seg_ids, lengths=lens)\n",
    "        n_infer, n_label, n_correct = metric.compute(lens,preds,labels)\n",
    "        metric.update(n_infer.numpy(),n_label.numpy(),n_correct.numpy())\n",
    "        precision, recall, f1_score = metric.accumulate()    \n",
    "    print(\"评估准确度: %.6f - 召回率: %.6f - f1得分: %.6f\" % (precision, recall, f1_score))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#模型训练\n",
    "global_step = 0\n",
    "for epoch in range(10):\n",
    "    #依次处理每批数据\n",
    "    for step, (input_ids, segment_ids, seq_lens, labels) in enumerate(train_loader, start=1):\n",
    "        #直接得到CRF Loss\n",
    "        loss = model(input_ids, token_type_ids=segment_ids,lengths=seq_lens, labels=labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"训练集的当前epoch:%d - step:%d\" % (epoch, step))\n",
    "            print(\"损失函数: %.6f\" % (avg_loss))\n",
    "        global_step += 1\n",
    "    #评估训练模型\n",
    "    evaluate(model, metric, dev_loader)\n",
    "    paddle.save(model.state_dict(),\n",
    "            './checkpoint/model_%d.pdparams'  % (global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#模型存储\n",
    "!mkdir bert_result\n",
    "#model.save_pretrained('./bert_result')\n",
    "#tokenizer.save_pretrained('./bert_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 8、加载和处理测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载测试数据\n",
    "def load_testdata(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            # next(fp)  # 没有header，不用Skip header\n",
    "            for line in fp.readlines():\n",
    "                ids, words = line.strip('\\n').split('\\001')\n",
    "                # 要预测的数据集没有label，伪造个O，不知道可以不 ，应该后面预测不会用label\n",
    "                labels=['O' for x in range(0,len(words))]\n",
    "                words_array=[]\n",
    "                for c in words:\n",
    "                    words_array.append(c)\n",
    "                yield words_array, labels\n",
    "    \n",
    "    #根据datafiles的数据类型，选择合适的处理方式\n",
    "    if isinstance(datafiles, str):#字符串，单个文件名称\n",
    "        #返回单个文件对应的单个数据集\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):#列表或元组，多个文件名称、\n",
    "        #返回多个文件对应的多个数据集\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载测试文件\n",
    "test_ds = load_testdata(datafiles=('./dataset/final_test.txt'))\n",
    "for i in range(10):\n",
    "    print(test_ds[i])\n",
    "#预处理编码\n",
    "test_ds.map(trans_func)\n",
    "print (test_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用paddle.io.DataLoader接口多线程异步加载数据。\n",
    "ignore_label = 1\n",
    "#创建Tuple对象，将多个批处理函数的处理结果连接在一起\n",
    "#因为数据集train_ds、dev_ds的每条数据包含4部分，所以Tuple对象中包含4个批处理函数，分别对应Token ID、Token Type、Len、Label\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): fn(samples)\n",
    "#paddle.io.DataLoader加载给定数据集，返回迭代器，每次迭代访问batch_size条数据\n",
    "#使用collate_fn定义所读取数据的格式\n",
    "test_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=50,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 9、Bert模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将标签编码转换为标签名称，组合成预测结果\n",
    "#ds：Bert模型生成的编码序列列表，decodes：待转换的标签编码列表，lens：句子有效长度列表，label_vocab：标签类型KV表\n",
    "def wgm_trans_decodes(ds, decodes, lens, label_vocab):\n",
    "    #将decodes和lens由列表转换为数组\n",
    "    decodes = [x for batch in decodes for x in batch]\n",
    "    lens = [x for batch in lens for x in batch]\n",
    "    #先使用zip形成元祖（编号, 标签），然后使用dict形成字典\n",
    "    id_label = dict(zip(label_vocab.values(), label_vocab.keys()))\n",
    "    #保存所有句子解析结果的列表\n",
    "    results=[]\n",
    "    #初始化编号\n",
    "    inNum = 1;\n",
    "    #逐个处理待转换的标签编码列表\n",
    "    for idx, end in enumerate(lens):\n",
    "        #句子单字构成的数组\n",
    "        sent_array = ds.data[idx][0][:end]\n",
    "        #句子单字标签构成的数组\n",
    "        tags_array = [id_label[x] for x in decodes[idx][1:end]]\n",
    "        #初始化句子和解析结果\n",
    "        sent = \"\";\n",
    "        tags = \"\";\n",
    "        #将字符串数组转换为单个字符串\n",
    "        for i in range(end-2):\n",
    "            #pdb.set_trace()\n",
    "            #单字直接连接，形成句子\n",
    "            sent = sent + sent_array[i]\n",
    "            #标签以空格连接\n",
    "            if i > 0:\n",
    "                tags = tags + \" \" + tags_array[i]\n",
    "            else:#第1个标签\n",
    "                tags = tags_array[i]\n",
    "        #构成结果串：编号+句子+标签序列，中间用“\\u0001”连接\n",
    "        current_pred = str(inNum) + '\\u0001' + sent + '\\u0001' + tags + \"\\n\"\n",
    "        #pdb.set_trace()\n",
    "        #添加到句子解析结果的列表\n",
    "        results.append(current_pred)\n",
    "        inNum = inNum + 1\n",
    "    return results\n",
    "\n",
    "#从标签编码中提取出地址元素\n",
    "#ds：ERNIE模型生成的编码序列列表，decodes：待转换的标签编码列表，lens：句子有效长度列表，label_vocab：标签类型KV表\n",
    "def wgm_parse_decodes(ds, decodes, lens, label_vocab):\n",
    "    #将decodes和lens由列表转换为数组\n",
    "    decodes = [x for batch in decodes for x in batch]\n",
    "    lens = [x for batch in lens for x in batch]\n",
    "    #先使用zip形成元祖（编号, 标签），然后使用dict形成字典\n",
    "    id_label = dict(zip(label_vocab.values(), label_vocab.keys()))\n",
    "    \n",
    "    #地址元素提取结果，每行是单个句子的地址元素列表\n",
    "    #例如：('朝阳区', 'district') ('小关北里', 'poi') ('000-0号', 'houseno')\n",
    "    outputs = []\n",
    "    for idx, end in enumerate(lens):\n",
    "        #句子单字构成的数组\n",
    "        sent = ds.data[idx][0][:end]\n",
    "        #句子单字标签构成的数组\n",
    "        tags = [id_label[x] for x in decodes[idx][1:end]]\n",
    "        #初始化地址元素名称和标签列表\n",
    "        sent_out = []\n",
    "        tags_out = []\n",
    "        #当前解析出来的地址元素名称\n",
    "        words = \"\"\n",
    "        #pdb.set_trace()\n",
    "        #逐个处理（单字, 标签）\n",
    "        #提取原理：如果当前标签是O，或者以B开头，那么说明遇到新的地址元素，需要存储已经解析出来的地址元素名称words\n",
    "        #然后，根据情况进行处理\n",
    "        for s, t in zip(sent, tags):\n",
    "            if t.startswith('B-') or t == 'O':#遇到新的地址元素\n",
    "                if len(words):#words非空，需要存储到sent_out\n",
    "                    sent_out.append(words)\n",
    "                if t == 'O':#标签为O，则直接存储标签\n",
    "                    #pdb.set_trace()\n",
    "                    tags_out.append(t)\n",
    "                else:#提取出标签\n",
    "                    tags_out.append(t.split('-')[1])\n",
    "                #新地址元素名称首字符\n",
    "                words = s\n",
    "            else:#完善地址元素名称\n",
    "                words += s\n",
    "        #处理地址串第1个地址元素时，sent_out长度为0，和tags_out的长度不同，需要补齐\n",
    "        if len(sent_out) < len(tags_out):\n",
    "            sent_out.append(words)\n",
    "        #按照（名称,标签）的形式组织地址元素，并且用空格分隔开\n",
    "        outputs.append(' '.join(\n",
    "            [str((s, t)) for s, t in zip(sent_out, tags_out)]))\n",
    "        #换行符号\n",
    "        outputs.append('\\n')\n",
    "    return outputs\n",
    "\n",
    "#使用Bert模型推理，并保存预测结果\n",
    "#data_loader：\n",
    "def wgm_predict_save(model, data_loader, ds, label_vocab, tagged_filename, element_filename):\n",
    "    pred_list = []\n",
    "    len_list = []\n",
    "    for input_ids, seg_ids, lens, labels in data_loader:\n",
    "        #pdb.set_trace()\n",
    "        preds = model(input_ids, seg_ids, lengths=lens)\n",
    "        preds = [pred[1:] for pred in preds.numpy()]\n",
    "        pred_list.append(preds)\n",
    "        len_list.append(lens)\n",
    "    #将标签编码转换为标签名称，组合成预测结果\n",
    "    predlist = wgm_trans_decodes(ds, pred_list, len_list, label_vocab)\n",
    "    #从标签编码中提取出地址元素\n",
    "    elemlist = wgm_parse_decodes(ds, pred_list, len_list, label_vocab)\n",
    "    #保存预测结果\n",
    "    with open(tagged_filename, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(predlist)\n",
    "    #保存地址元素\n",
    "    with open(element_filename, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(elemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载Bert模型\n",
    "#model = ppnlp.transformers.BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_classes=len(label_vocab))\n",
    "model_dict = paddle.load('checkpoint/model_300.pdparams')\n",
    "model.set_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#推理并预测结果\n",
    "wgm_predict_save(model, test_loader, test_ds, label_vocab, \"predict_wgm.txt\", \"element_wgm.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cffdb135bfca6e4a47037eb880605a7800674b0a8ee5d1d049ded49b6c5e4e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
