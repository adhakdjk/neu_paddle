{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-14T01:55:48.831309Z",
     "iopub.status.busy": "2022-09-14T01:55:48.830490Z",
     "iopub.status.idle": "2022-09-14T01:55:49.350781Z",
     "shell.execute_reply": "2022-09-14T01:55:49.349222Z",
     "shell.execute_reply.started": "2022-09-14T01:55:48.831271Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-14T01:55:49.354368Z",
     "iopub.status.busy": "2022-09-14T01:55:49.353482Z",
     "iopub.status.idle": "2022-09-14T01:55:49.914140Z",
     "shell.execute_reply": "2022-09-14T01:55:49.912641Z",
     "shell.execute_reply.started": "2022-09-14T01:55:49.354323Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-14T01:55:49.916628Z",
     "iopub.status.busy": "2022-09-14T01:55:49.915833Z",
     "iopub.status.idle": "2022-09-14T01:55:53.650375Z",
     "shell.execute_reply": "2022-09-14T01:55:53.649123Z",
     "shell.execute_reply.started": "2022-09-14T01:55:49.916589Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: 无法创建目录\"/home/aistudio/external-libraries\": 文件已存在\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting beautifulsoup4\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/9c/d8/909c4089dbe4ade9f9705f143c9f13f065049a9d5e7d34c828aefdd0a97c/beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/16/e3/4ad79882b92617e3a4a0df1960d6bce08edfb637737ac5c3f3ba29022e25/soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.11.1 soupsieve-2.3.2.post1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.3.2.post1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.11.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-14T01:55:53.652532Z",
     "iopub.status.busy": "2022-09-14T01:55:53.652023Z",
     "iopub.status.idle": "2022-09-14T01:55:53.658578Z",
     "shell.execute_reply": "2022-09-14T01:55:53.657784Z",
     "shell.execute_reply.started": "2022-09-14T01:55:53.652494Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import numpy as np\n",
    "import paddle \n",
    "import paddle.fluid as fluid\n",
    "from paddle.fluid.dygraph.nn import Embedding\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T01:55:53.661561Z",
     "iopub.status.busy": "2022-09-14T01:55:53.661113Z",
     "iopub.status.idle": "2022-09-14T01:57:18.335665Z",
     "shell.execute_reply": "2022-09-14T01:57:18.334365Z",
     "shell.execute_reply.started": "2022-09-14T01:55:53.661531Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download():\n",
    "    corpus_url =\"https://dataset.bj.bcebos.com/word2vec/text8.txt\"\n",
    "    web_request=requests.get(corpus_url)\n",
    "    corpus=web_request.content\n",
    "\n",
    "    with open(\"./text8.txt\",\"wb\") as f:\n",
    "        f.write(corpus)\n",
    "    f.close()\n",
    "\n",
    "download()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T01:57:18.337411Z",
     "iopub.status.busy": "2022-09-14T01:57:18.337092Z",
     "iopub.status.idle": "2022-09-14T01:57:18.554934Z",
     "shell.execute_reply": "2022-09-14T01:57:18.553807Z",
     "shell.execute_reply.started": "2022-09-14T01:57:18.337381Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "#读取text8数据\n",
    "def load_text8():\n",
    "    with open(\"./text8.txt\",\"r\") as f:\n",
    "        corpus =f.read().strip('/n')\n",
    "    f.close()\n",
    "    return corpus\n",
    "\n",
    "corpus = load_text8()\n",
    "\n",
    "\n",
    "print(corpus[:500])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T01:57:18.557075Z",
     "iopub.status.busy": "2022-09-14T01:57:18.556336Z",
     "iopub.status.idle": "2022-09-14T01:57:20.065593Z",
     "shell.execute_reply": "2022-09-14T01:57:20.064651Z",
     "shell.execute_reply.started": "2022-09-14T01:57:18.557041Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(corpus):\n",
    "\n",
    "    corpus=corpus.strip().lower()\n",
    "    corpus=corpus.split(\" \")#切分 空格\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = data_preprocess(corpus)\n",
    "print(corpus[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T01:57:20.067970Z",
     "iopub.status.busy": "2022-09-14T01:57:20.066888Z",
     "iopub.status.idle": "2022-09-14T01:57:26.868609Z",
     "shell.execute_reply": "2022-09-14T01:57:26.867477Z",
     "shell.execute_reply.started": "2022-09-14T01:57:20.067937Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totally 253854 different words in the corpus\n",
      "word the,its id 0,its word freq 1061396\n",
      "word of,its id 1,its word freq 593677\n",
      "word and,its id 2,its word freq 416629\n",
      "word one,its id 3,its word freq 411764\n",
      "word in,its id 4,its word freq 372201\n",
      "word a,its id 5,its word freq 325873\n",
      "word to,its id 6,its word freq 316376\n",
      "word zero,its id 7,its word freq 264975\n",
      "word nine,its id 8,its word freq 250430\n",
      "word two,its id 9,its word freq 192644\n",
      "word is,its id 10,its word freq 183153\n",
      "word as,its id 11,its word freq 131815\n",
      "word eight,its id 12,its word freq 125285\n",
      "word for,its id 13,its word freq 118445\n",
      "word s,its id 14,its word freq 116710\n",
      "word five,its id 15,its word freq 115789\n",
      "word three,its id 16,its word freq 114775\n",
      "word was,its id 17,its word freq 112807\n",
      "word by,its id 18,its word freq 111831\n",
      "word that,its id 19,its word freq 109510\n",
      "word four,its id 20,its word freq 108182\n",
      "word six,its id 21,its word freq 102145\n",
      "word seven,its id 22,its word freq 99683\n",
      "word with,its id 23,its word freq 95603\n",
      "word on,its id 24,its word freq 91250\n",
      "word are,its id 25,its word freq 76527\n",
      "word it,its id 26,its word freq 73334\n",
      "word from,its id 27,its word freq 72871\n",
      "word or,its id 28,its word freq 68945\n",
      "word his,its id 29,its word freq 62603\n",
      "word an,its id 30,its word freq 61925\n",
      "word be,its id 31,its word freq 61281\n",
      "word this,its id 32,its word freq 58832\n",
      "word which,its id 33,its word freq 54788\n",
      "word at,its id 34,its word freq 54576\n",
      "word he,its id 35,its word freq 53573\n",
      "word also,its id 36,its word freq 44358\n",
      "word not,its id 37,its word freq 44033\n",
      "word have,its id 38,its word freq 39712\n",
      "word were,its id 39,its word freq 39086\n",
      "word has,its id 40,its word freq 37866\n",
      "word but,its id 41,its word freq 35358\n",
      "word other,its id 42,its word freq 32433\n",
      "word their,its id 43,its word freq 31523\n",
      "word its,its id 44,its word freq 29567\n",
      "word first,its id 45,its word freq 28810\n",
      "word they,its id 46,its word freq 28553\n",
      "word some,its id 47,its word freq 28161\n",
      "word had,its id 48,its word freq 28100\n",
      "word all,its id 49,its word freq 26229\n"
     ]
    }
   ],
   "source": [
    "def build_dict(corpus):\n",
    "    #首先统计每个不同词的频率\n",
    "    word_fre_dict=dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_fre_dict:\n",
    "            word_fre_dict[word]=0\n",
    "        word_fre_dict[word]+=1\n",
    "            #高频词排序靠前 一般来说高频词是 i等代词，频率低的次往往是一些名词，如，nlp\n",
    "            \n",
    "    word_fre_dict=sorted(word_fre_dict.items(),key= lambda x:x[1],reverse=True)\n",
    "    \n",
    "    print(word_fre_dict)\n",
    "#词到id的映射关系：word2id_dict\n",
    "#每个id出现的频率：word2id_freq\n",
    "#每个id到词典映射关系:id2word_dict\n",
    "    word2id_dict=dict()\n",
    "    word2id_freq=dict()\n",
    "    id2word_dict=dict() \n",
    "#按照频率，从高到低开始遍历每个单词，提供唯一id\n",
    "    for word,freq in word_fre_dict:\n",
    "        curr_id=len(word2id_dict)\n",
    "        word2id_dict[word]=curr_id\n",
    "        word2id_freq[word2id_dict[word]]=freq\n",
    "        id2word_dict[curr_id]=word\n",
    "\n",
    "    return word2id_freq,word2id_dict,id2word_dict\n",
    "\n",
    "\n",
    "\n",
    "word2id_freq,word2id_dict,id2word_dict=build_dict(corpus)\n",
    "vocab_size=len(word2id_freq)\n",
    "print(\"there are totally %d different words in the corpus\" % vocab_size)\n",
    "\n",
    "for _,(word,word_id) in zip(range(50),word2id_dict.items()):\n",
    "    print(\"word %s,its id %d,its word freq %d\" % (word,word_id,word2id_freq[word_id]))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T01:57:26.870805Z",
     "iopub.status.busy": "2022-09-14T01:57:26.870081Z",
     "iopub.status.idle": "2022-09-14T01:57:30.042603Z",
     "shell.execute_reply": "2022-09-14T01:57:30.041470Z",
     "shell.execute_reply.started": "2022-09-14T01:57:26.870772Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 tokens in the corpus\n",
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580, 0, 194, 10, 190, 58, 4, 5, 10712, 214, 6, 1324, 104, 454, 19, 58, 2731, 362, 6, 3672, 0]\n"
     ]
    }
   ],
   "source": [
    "#语料转换成id序列\n",
    "\n",
    "def convert_corpus_to_id(corpus,word2id_dict):\n",
    "    corpus=[word2id_dict[word] for word in corpus]\n",
    "    return corpus\n",
    "\n",
    "corpus = convert_corpus_to_id(corpus,word2id_dict)\n",
    "\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T01:57:30.044723Z",
     "iopub.status.busy": "2022-09-14T01:57:30.043983Z",
     "iopub.status.idle": "2022-09-14T01:57:30.050018Z",
     "shell.execute_reply": "2022-09-14T01:57:30.049092Z",
     "shell.execute_reply.started": "2022-09-14T01:57:30.044689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#二次采样算法，优化词分布 强化训练效果\n",
    "\n",
    "def subsampling(corpus,word2id_freq):\n",
    "    #这个discard函数决定了一个词会不会被替换，这个函数具体有随机性，每次提调用结构不同\n",
    "    #如果一个词的频率很大，那么它被遗弃的概率很大\n",
    "    def discard(word_id):\n",
    "        return random.uniform(0,1)< 1-math.sqrt(le-4/word2id_freq * len(corpus))\n",
    "\n",
    "    corpus = subsampling(corpus,word2id_freq)\n",
    "    return corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T02:01:01.167859Z",
     "iopub.status.busy": "2022-09-14T02:01:01.166556Z",
     "iopub.status.idle": "2022-09-14T02:01:01.176739Z",
     "shell.execute_reply": "2022-09-14T02:01:01.175840Z",
     "shell.execute_reply.started": "2022-09-14T02:01:01.167820Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#max_window_size代表了最大word_size打大小，程序会根据max_window_size从左到右扫描整个语料\n",
    "#negative_sample_num代表了对于每个正样本，我么需要随机采样多少负样本用于训练，\n",
    "#一般来说，negative_sample_num越大，训练效果越稳定，但训练速度越慢\n",
    "\n",
    "import random\n",
    "def build_data(corpus,word2id_dict,word2id_freq,max_window_size=3,negative_sample_num=4):\n",
    "    #使用一个list存储处理好的的数据\n",
    "    dataset=[]\n",
    "\n",
    "    #从左到右开始枚举中心点的位置\n",
    "    for center_word_idx in range(len(corpus)):\n",
    "        #以max_window_size为最大值,随机采样一个window_size\n",
    "        window_size=random.randint(1,max_window_size)\n",
    "        #当前中心词为中心，左右两侧在window_size内的词都可以看成是正样本\n",
    "        center_word=corpus[center_word_idx]\n",
    "        \n",
    "        #以当前中心词为中心，左右两侧在window_size内都可以看做是正样本\n",
    "        postive_word_range=(max(0,center_word_idx-window_size),min(len(corpus)-1,center_word_idx+ window_size))\n",
    "        postive_word_candidates=[corpus[idx] for idx in range(postive_word_range[0],postive_word_range[1]+1) if idx!=center_word_idx]\n",
    "\n",
    "\n",
    "        #对于每个正样本随机采样n_s_n个负样本\n",
    "        \n",
    "        for postive_word in postive_word_candidates:\n",
    "            #先把（中心词，正样本，label==1）三元数组放入dataset中\n",
    "            #这里label=1表示这个样本是正样本\n",
    "            dataset.append((center_word,postive_word,1))\n",
    "\n",
    "            #负采样\n",
    "            i=0\n",
    "            while i<negative_sample_num:\n",
    "                negative_word_candidate=random.randint(0,vocab_size-1)\n",
    "\n",
    "                if negative_word_candidate not in postive_word_candidates:\n",
    "                    #把（中心词，正样本,label=0）的三元数组放入dataset中,\n",
    "                    #这里lable=0代表负样本\n",
    "\n",
    "                    dataset.append((center_word,negative_word_candidate,0))\n",
    "                    i+=1\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T02:01:02.413047Z",
     "iopub.status.busy": "2022-09-14T02:01:02.411656Z",
     "iopub.status.idle": "2022-09-14T02:01:02.421936Z",
     "shell.execute_reply": "2022-09-14T02:01:02.420972Z",
     "shell.execute_reply.started": "2022-09-14T02:01:02.413004Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#构造mini_batch 准备对模型进行训练\n",
    "#我们将不同类型的数据放到不同的tensor礼，便于神经网络进行处理\n",
    "#并通过 numpy的array函数 构造不同的tensor 把这些tensor送入神经网络当周\n",
    "\n",
    "\n",
    "def build_batch(dataset,batch_size,epoch_num):\n",
    "    #center_word_batch缓存batch_size个中心词\n",
    "    center_word_batch=[]\n",
    "    #target_word_batch缓存batch_size个目标词(正样本或者负样本)\n",
    "    target_word_batch=[]\n",
    "    #label_batch缓存batch_size个0或1标签用于模型训练\n",
    "    label_batch=[]\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        #每个新epoch_num之前，都对数据进行一次随机打乱，提高训练效果\n",
    "        random.shuffle(dataset)\n",
    "\n",
    "        for center_word,target_word,label in dataset:\n",
    "            #遍历dataset中的每个样本，并将这些数据送到不同的tensor中\n",
    "\n",
    "            center_word_batch.append([center_word])\n",
    "            target_word_batch.append([target_word])\n",
    "            label_batch.append(label)\n",
    "\n",
    "            #当样本积攒到一个batch_size后，把数据都返回\n",
    "            #使用np的array函数把list封装成tensor\n",
    "            #并使用python的迭代器机制，把数据yield\n",
    "            #使用迭代器的好处是节省内存\n",
    "\n",
    "            if len(center_word_batch) == batch_size:\n",
    "                yield np.array(center_word_batch).astype(\"int64\"),\\\n",
    "                    np.array(target_word_batch).astype(\"int64\"),\\\n",
    "                    np.array(label_batch).astype(\"float32\")\n",
    "                center_word_batch=[]\n",
    "                target_word_batch=[]\n",
    "                label_batch=[]\n",
    "            \n",
    "        if len(center_word_batch) >0:\n",
    "            yield np.array(center_word_batch).astype(\"int64\"),\\\n",
    "                np.array(target_word_batch).astype(\"int64\"),\\\n",
    "                np.array(label_batch).astype(\"float32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T02:01:04.251524Z",
     "iopub.status.busy": "2022-09-14T02:01:04.250924Z",
     "iopub.status.idle": "2022-09-14T02:01:04.259768Z",
     "shell.execute_reply": "2022-09-14T02:01:04.258844Z",
     "shell.execute_reply.started": "2022-09-14T02:01:04.251486Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#定义skip-gram网络\r\n",
    "class SkipGram(fluid.dygraph.Layer):\r\n",
    "    def __init__(self,vocab_size,embedding_size,init_scale=0.1):\r\n",
    "\r\n",
    "        super(SkipGram,self).__init__()\r\n",
    "        self.vocab_size=vocab_size\r\n",
    "        self.embedding_size=embedding_size\r\n",
    "#构造词向量参数\r\n",
    "        self.embedding=Embedding(\r\n",
    "            size=[self.vocab_size,self.embedding_size],\r\n",
    "            dtype=\"float32\",\r\n",
    "            param_attr=fluid.ParamAttr(\r\n",
    "                name=\"embedding_para\",\r\n",
    "                initializer=fluid.initializer.UniformInitializer(\r\n",
    "                    low=-0.5/embedding_size,high=0.5/embedding_size\r\n",
    "                )\r\n",
    "            )\r\n",
    "        )\r\n",
    "\r\n",
    "\r\n",
    "        self.embedding_out=Embedding(\r\n",
    "            size=[self.vocab_size,self.embedding_size],\r\n",
    "            dtype='float32',\r\n",
    "            param_attr=fluid.ParamAttr(\r\n",
    "                name=\"embedding_out_para\",\r\n",
    "                initializer=fluid.initializer.UniformInitializer(\r\n",
    "                    low=-0.5/embedding_size,\r\n",
    "                )\r\n",
    "            )\r\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T02:01:05.359170Z",
     "iopub.status.busy": "2022-09-14T02:01:05.358431Z",
     "iopub.status.idle": "2022-09-14T02:01:05.366208Z",
     "shell.execute_reply": "2022-09-14T02:01:05.365199Z",
     "shell.execute_reply.started": "2022-09-14T02:01:05.359131Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#前馈\n",
    "def forward(self,center_words,target_words,label):\n",
    "    center_words_emb=self.embedding(center_words)\n",
    "    target_words_emb=self.embedding_out(target_words)\n",
    "    #点乘运算\n",
    "    word_sim= fluid.layers.elementwise_mul(center_words_emb,target_words_emb)\n",
    "    word_sim=fluid.layers.reduce_sum(word_sim,dim=-1)\n",
    "    word_sim=fluid.layers.reshape(word_sim,shape=[-1])\n",
    "    pred=fluid.layers.sigmoid(word_sim)\n",
    "\n",
    "    #sigmoid和cross entropy合成以后还可以尽可能的优化。\n",
    "    loss=fluid.layers.sigmod_crosss_entropy_with_logits(word_sim,label)\n",
    "    loss=fluid.layers.reduce_mean(loss)\n",
    "\n",
    "    return pred,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T02:01:06.909009Z",
     "iopub.status.busy": "2022-09-14T02:01:06.908186Z",
     "iopub.status.idle": "2022-09-14T02:01:06.916633Z",
     "shell.execute_reply": "2022-09-14T02:01:06.915657Z",
     "shell.execute_reply.started": "2022-09-14T02:01:06.908972Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=512\r\n",
    "epoch_num=3\r\n",
    "embedding_size=200\r\n",
    "step=0\r\n",
    "learning_rate=0.001\r\n",
    "\r\n",
    "#embed 学习到的embedding参数\r\n",
    "def get_similar_tokens(query_token,k,embed):\r\n",
    "    W=embed.numpy()\r\n",
    "    X=W[word2id_dict[query_token]]\r\n",
    "    cos=np.dot(W,X)/nq.sqrt(np.sum(W*W,axis=1)* np.sum(x*x)+1e-9)\r\n",
    "    flat=cos.flatten()\r\n",
    "    indices=np.argpartition(flat,-k)[-k:]\r\n",
    "    indices=indices[np.argsort(-flat[indices])]\r\n",
    "    for i in indices:\r\n",
    "        print('for word %s,the similar word is %s' % (query_token,str(id2word_dict[i])))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T02:01:08.397403Z",
     "iopub.status.busy": "2022-09-14T02:01:08.396789Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with fluid.dygraph.guard(fluid.CPUPlace()):\r\n",
    "    dataset=build_data(corpus,word2id_dict,word2id_freq,max_window_size=3,negative_sample_num=4)\r\n",
    "    \r\n",
    "    skip_gram_model=SkipGram(vocab_size,embedding_size)\r\n",
    "\r\n",
    "    adam=fluid.optimizer.AdamOptimizer(learning_rate=learning_rate,parameter_list=skip_gram_model.parameters())\r\n",
    "\r\n",
    "    for center_words,target_words,label in build_batch(dataset,batch_size,epoch_num):\r\n",
    "        center_words_vars=fluid.dygraph.to_variable(center_words)\r\n",
    "        target_words_vars=fluid.dygraph.to_variable(target_words)\r\n",
    "        label_var=fluid.dygraph.to_variable(label)\r\n",
    "\r\n",
    "        pred,loss-skip_gram_model(center_words_vars,target_words_vars,label_var)\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        adam.minimize(loss)\r\n",
    "\r\n",
    "        skip_gram_model.clear_gradients()\r\n",
    "\r\n",
    "        step+=1\r\n",
    "        if step %100==0:\r\n",
    "            print(\"step %d,loss %.3f\" % (step,loss.numpy()[0]))\r\n",
    "\r\n",
    "        if step %10000 ==0:\r\n",
    "            get_similar_tokens(\"one\",5,skip_gram_model.embedding.weight)\r\n",
    "            get_similar_tokens('she',5,skip_gram_model.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
